
# 🎨 Fine-tuning text-to-image diffusion models for personalization and subject-driven generation
### Presentation: [Personalization of Diffusion Models with 🧨Diffusers](https://docs.google.com/presentation/d/1m-ZaXuB0dDcg77EUaS6cnDaYED89JYviYv-fYRvu8yU/edit?usp=sharing)

## 📚 Workshop description
During the workshop you will get familiar with different fine-tuning techniques for text-to-image models, and learn how to easily teach a diffusion model a concept of your choosing  (special style, a pet, faces etc) with as little as 3 images depicting your concept. 

## 🛠️ Requirements
Python >= 3.10, acquaintance with Diffusion models, Text-to-Image models. 
> **NOTE** 💡 While we will briefly go over diffusion models and |  
specifically Stable Diffusion, we will not get into detail, and assume some familiarity with diffusion process and architecture of stable diffusion models.

> **TIP** 💌 If you're not familiar with diffusion models but interested in doing this workshop, checkout this (completely open sourced) [introductory diffusion class](https://github.com/huggingface/diffusion-models-class) 🤓

## ▶️ Usage
* Clone the repository
* Start jupyter lab and navigate to the workshop folder
* Open the first workshop notebook
* * [Option1] Install requirements with `pip install -r requirements.txt`
* * [Option2] Run the `Setup` cells in the notebook 

## 🎬 Video record
Re-watch [this YouTube stream](https://www.youtube.com/live/f9FWJ9UjZ-U)

## 🤝 Credits
This workshop was set up by @pyladiesams and @linoytsaban
