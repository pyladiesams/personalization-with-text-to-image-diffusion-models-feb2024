
# ğŸ¨ Fine-tuning text-to-image diffusion models for personalization and subject-driven generation
### Presentation: [Personalization of Diffusion Models with ğŸ§¨Diffusers](https://docs.google.com/presentation/d/1m-ZaXuB0dDcg77EUaS6cnDaYED89JYviYv-fYRvu8yU/edit?usp=sharing)

## ğŸ“š Workshop description
During the workshop you will get familiar with different fine-tuning techniques for text-to-image models, and learn how to easily teach a diffusion model a concept of your choosing  (special style, a pet, faces etc) with as little as 3 images depicting your concept. 

## ğŸ› ï¸ Requirements
Python >= 3.10, acquaintance with Diffusion models, Text-to-Image models. 
> **NOTE** ğŸ’¡ While we will briefly go over diffusion models and |  
specifically Stable Diffusion, we will not get into detail, and assume some familiarity with diffusion process and architecture of stable diffusion models.

> **TIP** ğŸ’Œ If you're not familiar with diffusion models but interested in doing this workshop, checkout this (completely open sourced) [introductory diffusion class](https://github.com/huggingface/diffusion-models-class) ğŸ¤“

## â–¶ï¸ Usage
* Clone the repository
* Start jupyter lab and navigate to the workshop folder
* Open the first workshop notebook
* * [Option1] Install requirements with `pip install -r requirements.txt`
* * [Option2] Run the `Setup` cells in the notebook 

## ğŸ¬ Video record
Re-watch [this YouTube stream](https://www.youtube.com/live/f9FWJ9UjZ-U)

## ğŸ¤ Credits
This workshop was set up by @pyladiesams and @linoytsaban
