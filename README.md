
# üé® Fine-tuning text-to-image diffusion models for personalization and subject-driven generation
### Presentation: [Personalization of Diffusion Models with üß®Diffusers](https://docs.google.com/presentation/d/1m-ZaXuB0dDcg77EUaS6cnDaYED89JYviYv-fYRvu8yU/edit?usp=sharing)

## üìö Workshop description
During the workshop you will get familiar with different fine-tuning techniques for text-to-image models, and learn how to easily teach a diffusion model a concept of your choosing  (special style, a pet, faces etc) with as little as 3 images depicting your concept. 

## üõ†Ô∏è Requirements
Python >= 3.10, acquaintance with Diffusion models, Text-to-Image models. 
> [!NOTE] While we will briefly go over diffusion models and |  
specifically Stable Diffusion, we will not get into detail, and assume some familiarity with diffusion process and architecture of stable diffusion models.
> [!TIP] If you're not familiar with diffusion models but interested in doing this workshop, checkout this (completely open sourced) [introductory diffusion class](https://github.com/huggingface/diffusion-models-class) ü§ì

## ‚ñ∂Ô∏è Usage
* Clone the repository
* Start jupyter lab and navigate to the workshop folder
* Open the first workshop notebook
* * [Option1] Install requirements with `pip install -r requirements.txt`
* * [Option2] Run the `Setup` cells in the notebook 

## üé¨ Video record
Re-watch [this YouTube stream](https://www.youtube.com/live/f9FWJ9UjZ-U)

## ü§ù Credits
This workshop was set up by @pyladiesams and @linoytsaban
